TORCH_DISTRIBUTED_DEBUG=DETAIL PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python AZFUSE_USE_FUSE=0 QD_USE_LINEIDX_8B=0 NCCL_ASYNC_ERROR_HANDLING=0 CUDA_VISIBLE_DEVICES='0,1' python -m torch.distributed.launch --nproc_per_node=2 --master_port=12315 finetune_sdm_yaml.py \
--cf config/ref_attn_clip_combine_controlnet_attr_pretraining/coco_S256_xformers_tsv_strongrand5.py \
--do_train --root_dir /home/nfs/jsh/DisCo \
--local_train_batch_size 16 --local_eval_batch_size 16 --log_dir exp/pretrain_5.0_dino_hd_gpu2_resume \
--epochs 600 --deepspeed --eval_step 500 --save_step 500 --gradient_accumulate_steps 1 \
--learning_rate 1e-4 --fix_dist_seed --loss_target "noise" \
--train_yaml /home/nfs/jsh/HOME/VITON-hd-resized/train/tsv/train.yaml \
--val_yaml /home/nfs/jsh/HOME/VITON-hd-resized/try/tsv/val.yaml \
--unet_unfreeze_type "all"  --ref_null_caption False \
--combine_clip_local --combine_use_mask --viton_hd --no_smpl \
--conds "masks" --max_eval_samples 2000 --strong_aug_stage1 --node_split_sampler 0 \
--stage1_pretrain_path /home/nfs/jsh/DisCo/exp/pretrain_3.0_1_dino_hd/53499.pth/mp_rank_00_model_states.pt 